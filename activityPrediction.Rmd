---
title: "Activity prediction using sensor data"
author: "Dominic Tabeta"
date: "May 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

This project uses sensor data taken from activity sensors labelled with activity types and trains a machine learning model that predicts activity types using this sensor data as predictors. We also carry out cross validation within the training set.

## Data Source
The source for the training data set: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The source for the test data set: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project originated from here:
http://groupware.les.inf.puc-rio.br/har

## Initial Library setup

```{r initialsetup,echo=TRUE}
library(caret)
library(rpart)
library(randomForest)
library(rpart.plot)
library(rattle)
```

## Data loading

```{r loadingandpreprocessing}
setwd("~/datascience/pmlproject")
trainurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainfile <- "../data/pml-training.csv"

if (!file.exists(trainfile)) {
        download.file(trainurl,destfile=trainfile,quiet=TRUE,mode="wb")
}
testfile <- "../data/pml-testing.csv"
if (!file.exists(testfile)) {
        download.file(testurl,destfile=testfile,quiet=TRUE,mode="wb")
}

traindata <- read.csv((trainfile), na.strings=c("NA","#DIV/0!",""))
testdata <- read.csv((testfile), na.strings=c("NA","#DIV/0!",""))
```

## Reproducibility

FOr reproducibility we set the random number seed here
``` {r setseed}
set.seed(123)
```

## Partitioning the training data set

For assessment of the machine learning models prior to application on the final test set I partition the training set here. The training-test split selected is 80-20.
``` {r partition}
selecttrain<-createDataPartition(y=traindata$classe,p=0.8,list=FALSE)
mytrain<-traindata[selecttrain,]
mytest<-traindata[-selecttrain,]
```

## Data Processing/Cleaning
We remove the index column and any variables that have zero variance from the mytrain and mytest datasets. Also we remove the columns that have over 97% missing values. Zero variance variables are of no use as predictors. 
``` {r cleaning}
nzv <- nearZeroVar(mytrain, saveMetrics=TRUE)
mytrainclean<-mytrain[,row.names(nzv[nzv$percentUnique>0&nzv$percentUnique<100,])]


nacheck<-apply(mytrainclean,2,is.na)

nasum<-apply(nacheck,2,sum)
naprop<-nasum/nrow(mytrainclean)

mytrainclean<-mytrainclean[,names(naprop[naprop<.95])]

mytestclean<-mytest[,colnames(mytrainclean)]
testfinal<-testdata[,colnames(mytrainclean[-59])]

dim(mytrainclean)
dim(mytestclean)
dim(testfinal)
```

## Prediction model using Decision Trees (rpart)
For the first prediction model we apply the rpart function to create a decision tree, and afterwards call fancyRPartPlot to visualise the tree. I chose rpart as one of the prediction models due to its easier interpretability which makes it a good starting point.
``` {r rpartmodel}
model1 <- rpart(classe ~ ., data=mytrainclean, method="class")
fancyRpartPlot(model1)
```

Below we assess the performance of the decision tree using the confusionMatrix() function.
``` {r predict1}
prediction1 <- predict(model1, mytestclean, type = "class")
confusionMatrix(prediction1, mytest$classe)
accuracy1<-confusionMatrix(prediction1, mytest$classe)$overall["Accuracy"]*100
oose1<-100-accuracy1
```

The confusion matrix results show an accuracy of `r accuracy1`% for the decision tree prediction model created. The expected out of sample error for this prediction model is `r oose1`%.

## Prediction model using Random Forest
For the second prediction model we apply the randomForest() function to create a random forest prediction model. Randomforest is chosen for its better performance than decision trees. As for the first model using rpart, we then assess its performance using the confusionMatrix() function.
``` {r randomForest}
model2 <- randomForest(classe ~. , data=mytrainclean)
prediction2 <- predict(model2, mytestclean, type = "class")
confusionMatrix(prediction2, mytest$classe)
accuracy2<-confusionMatrix(prediction2, mytest$classe)$overall["Accuracy"]*100
oose2<-100-accuracy2
```


The confusion matrix results show an accuracy of `r accuracy2`% for the random forest prediction model created, which is more accurate than the decision tree prediction model created earlier. The expected out of sample error for this model is `r oose2`%.

## Applying predictor models to the given test data

We apply the two prediction models created to the given test data set to produce two sets of predictions. The expected out of set error for each model would be 1-Accuracy for each model's accuracy result.
```{r finaltests}
prediction3<-predict(model1,testfinal,type="class")
prediction3

levels(testfinal$cvtd_timestamp)<-levels(mytrainclean$cvtd_timestamp)
levels(testfinal$new_window)<-levels(mytrainclean$new_window)
prediction4<-predict(model2,testfinal,type="class")
prediction4
```

## Results
Comparing the performance of the two prediction models, we find that the random forest model has greater accuracy (`r accuracy2`%) than the decision tree model (`r accuracy1`%). Despite reducing the amount of predictors from 159 to 58, we were able to achieve a high degree of accuracy using a random forest method.